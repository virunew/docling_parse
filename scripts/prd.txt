<context>
# Overview
The Docling JSON Parser is a Python tool designed to extract structured content from JSON files generated by the Docling service. It solves the problem of converting complex, deeply nested document data into a standardized format suitable for database ingestion. This tool is valuable for organizations that need to extract, organize, and store document content from PDFs processed by Docling, making the information searchable and accessible in downstream applications.

# Core Features
## Content Extraction and Classification
- What it does: Parses JSON files conforming to DoclingDocument v1.3.0 schema and extracts text paragraphs, tables, and images while identifying non-content "furniture" elements.
- Why it's important: Distinguishes meaningful content from decorative or structural elements, ensuring only relevant information is processed.
- How it works: Uses element labels, content layers, and structural hierarchy to categorize content appropriately.

## Hierarchical Context Generation
- What it does: Creates a hierarchical breadcrumb for each content chunk based on section headers.
- Why it's important: Preserves document structure and enables improved search, navigation, and understanding of content relationships.
- How it works: Scans backward through the document sequence to find preceding section headers at each level, building a complete breadcrumb path.

## Metadata Enrichment
- What it does: Enhances content chunks with contextual information including page numbers, bounding box coordinates, captions, and surrounding text.
- Why it's important: Provides rich context for each content piece, improving searchability and enabling reconstruction of the original document experience.
- How it works: Extracts metadata from provenance information and analyzes document flow to determine relationships between elements.

## External Image Management
- What it does: Decodes image data URIs and saves images as binary files to an external location.
- Why it's important: Allows efficient storage and retrieval of document images without bloating the database.
- How it works: Extracts base64-encoded image data, determines appropriate file format, and saves to a configurable storage location with appropriate naming.

## Standardized Output Generation
- What it does: Formats extracted content and metadata into a specific JSON structure suitable for database ingestion.
- Why it's important: Ensures compatibility with the target database schema and downstream applications.
- How it works: Maps extracted information to predefined fields while maintaining relationships and hierarchy.

## Database Integration
- What it does: Directly saves processed document chunks to a configurable database system.
- Why it's important: Eliminates additional steps in the data pipeline, providing immediate data availability.
- How it works: Provides configurable adapters for PostgreSQL and MongoDB, with initial implementation focused on PostgreSQL integration.

# User Experience
## Developer Persona
- Python developers working with document processing pipelines who need to extract structured data from complex document formats.
- Data engineers integrating document content into search systems or knowledge bases.
- Technical users familiar with JSON processing and database schemas.

## Key User Flows
- User provides a DoclingDocument JSON file and storage location for images.
- Parser extracts content, saves images externally, and generates standardized JSON output.
- User receives structured data ready for database ingestion or can choose direct database insertion.
- Minimal configuration required beyond specifying input/output paths, image storage location, and database connection parameters.

## UX Considerations
- Command-line interface with clear argument requirements.
- Informative progress and error messages during processing.
- Structured output that clearly maps to database schema.
- Documentation of output format and field mappings.
- Database configuration options with sensible defaults.
</context>
<PRD>
# Technical Architecture

## System Components
- **Parser Core**: Python module implementing the document parsing logic.
- **Element Resolver**: Resolves `$ref` pointers to build a complete element map.
- **Main Parsing Program (parse_main.py)**: Entry point for the application that handles PDF parsing with docling and generates the element dictionary.
- **Image Extraction Module**: Uses docling library to extract images and maintain links to their references.
- **OCR Processor**: Extracts text from images in the document and formats it in markdown.
- **Sequence Flattener**: Determines correct reading order of document elements.
- **Breadcrumb Generator**: Creates hierarchical path for each content chunk.
- **Image Handler**: Decodes and saves images to external storage.
- **Output Formatter**: Generates the final JSON structure.
- **CLI Interface**: Processes command-line arguments and handles I/O.
- **Database Adapter**: Manages connection and insertion of document chunks into configurable database systems.

## Data Models
- **Input**: DoclingDocument v1.3.0 JSON schema with nested element references.
  - Contains texts, pictures, tables, groups elements with self_ref and $ref cross-references
  - Includes provenance (prov) information with page numbers and bounding boxes
  - Distinguishes between furniture and main body content via content_layer attribute
- **Internal**: Flattened sequence of elements with resolved references.
  - Elements ordered in correct reading sequence
  - Cross-references fully resolved into an element map
  - Hierarchical structure preserved for breadcrumb generation
- **Output**: Standardized JSON structure with chunks, furniture, and metadata.
  - Top-level keys: chunks, furniture, source_metadata
  - Each chunk representing a distinct content element from the document
- **Chunk Object**: Structured data mapping to the `fusa_library` table schema.
  - Contains 30+ fields including content_type, coords, text_block, and metadata
- **Database Schema**: 
  - PostgreSQL implementation matches fields in the `fusa_library` table
  - Future MongoDB option would store documents in a chunks collection

## APIs and Integrations
- **File System Integration**: Reads input JSON and writes output JSON and images.
- **Base64 Decoding**: Converts image data URIs to binary image files.
- **JSON Processing**: Handles complex nested structures with cross-references.
- **Docling Library Integration**: Uses docling to extract images from PDF documents.
- **OCR Integration**: Uses OCR technology to extract text from images.
- **Database Integration**: Direct insertion into configurable database backends (PostgreSQL initially, with MongoDB support planned).

## Infrastructure Requirements
- Python 3.x runtime environment.
- File system access for reading input and writing output.
- External storage location for image files with appropriate permissions.
- OCR library (Tesseract or similar) for image text extraction.
- Docling library for PDF processing and image extraction.
- PostgreSQL database for direct insertion (initial implementation).
- Database connection libraries: psycopg2 for PostgreSQL, pymongo for future MongoDB support.
- Minimal memory requirements for typical document sizes.

# Development Roadmap

## MVP Requirements
1. **Core Parser Implementation**:
   - JSON parsing with reference resolution
   - Content type identification and extraction
   - Breadcrumb generation (non-truncated, using full header text)
   - Basic metadata extraction
   - Element map building functionality
   - Document sequence flattening
   
2. **Main Parsing Program (parse_main.py)**:
   - Process PDF documents using the docling library
   - Extract text and images into usable format
   - Generate the dictionary structure needed by the rest of the application
   - Integrate with the element map builder and other components
   - Command-line interface for input/output specification

3. **Image Processing and OCR**:
   - Image extraction using docling library
   - Link images to their references in the document
   - Extract text from images using OCR
   - Format text blocks in markdown with three sections:
     * Preceding text (text appearing before the image)
     * Image text (text extracted from the image via OCR)
     * Succeeding text (text appearing after the image)
   - Integrate OCR results into the element map

4. **External Image Storage**:
   - Image data URI decoding from base64
   - MIME type detection and appropriate file extension selection
   - File saving with unique naming based on doc_id and block_id
   - Path recording in output's external_files field
   - Directory creation and collision handling
   
5. **Output Generation**:
   - Structured JSON formatting per documented schema
   - Database schema field mapping to fusa_library table 
   - Configurable schema field mapping to support multiple mappings 
   - Special fields population with breadcrumbs and metadata
   - Basic error handling for file operations and JSON parsing
   - Command-line interface with required arguments

6. **Database Integration (PostgreSQL)**:
   - Database connection configuration via command-line arguments
   - Connection pooling and error handling
   - Direct insertion into PostgreSQL database matching fusa_library schema
   - Table creation or validation for new installations
   - Batch insertion for performance optimization
   - Transaction management for reliable operation

## Future Enhancements
1. **Performance Optimization**:
   - Parallel processing for multiple documents
   - Streaming parser for large files
   - Memory usage improvements for large documents
   - Bulk database operations for improved throughput
   
2. **Advanced Content Processing**:
   - Table content extraction improvements
   - Image analysis for additional metadata (dimensions, format details)
   - Better handling of nested lists and complex layouts
   - Enhanced OCR accuracy for complex images
   
3. **Integration Capabilities**:
   - MongoDB database adapter implementation
   - API wrapper for programmatic usage
   - Webhook support for processing notifications
   - Progress reporting for long-running processes
   
4. **Quality Improvements**:
   - Comprehensive input validation
   - Enhanced error reporting and diagnostics
   - Processing statistics and summaries
   - Validation against target database schema
   - Testing with diverse document types

# Logical Dependency Chain
1. **Foundation Components**:
   - Element map builder (to resolve all references)
   - Main parsing program (parse_main.py) to process PDF documents
   - Image extraction using docling library
   - Document sequence flattener (to determine reading order)
   - Basic metadata extraction from provenance data
   
2. **Content Processing Pipeline**:
   - Text content extraction with proper labels
   - Image text extraction using OCR
   - Table content formatting into grid structures
   - Image handling and external storage implementation
   - Furniture element identification and extraction
   
3. **Context Enhancement**:
   - Breadcrumb generation with hierarchical section headers
   - Context snippet extraction (a paragraph before and after or 100 words, whichever is larger)
   - Caption association with tables and images
   - Bounding box coordinate conversion
   
4. **Output Structure**:
   - Chunk assembly with all required fields
   - Database field mapping according to specification
   - JSON formatting with proper nesting
   - Metadata object creation and serialization
   
5. **Database Integration**:
   - Database configuration handling
   - PostgreSQL connection and table validation
   - Data insertion with proper field mapping
   - Transaction and error management

6. **CLI and Error Handling**:
   - Command-line interface with argument parsing
   - Validation and error reporting
   - User feedback mechanisms
   - File I/O handling
   - Database connection error handling

# Risks and Mitigations

## Technical Challenges
- **Complex Document Structures**: Some documents may have unexpected nesting or reference patterns.
  - *Mitigation*: Implement robust error handling and fallback behavior for unusual structures.
  - *Mitigation*: Add diagnostic logging for reference resolution issues.

- **Image Handling**: Base64 decoding and file saving may encounter format or permission issues.
  - *Mitigation*: Include detailed error reporting and graceful failure modes for image processing.
  - *Mitigation*: Validate MIME types before processing to avoid corruption.

- **OCR Accuracy**: Text extraction from images may be unreliable for low-quality or complex images.
  - *Mitigation*: Implement confidence thresholds and fallback text for poor OCR results.
  - *Mitigation*: Consider multiple OCR engines for improved accuracy.

- **Breadcrumb Accuracy**: Complex documents may have unclear section hierarchies.
  - *Mitigation*: Implement multiple strategies for breadcrumb generation with sensible defaults.
  - *Mitigation*: Ensure full header text is preserved (not truncated).

- **Database Connectivity**: Database connections may fail due to misconfiguration or network issues.
  - *Mitigation*: Implement robust error handling with clear error messages.
  - *Mitigation*: Support fallback to JSON output when database connection fails.
  - *Mitigation*: Provide connection testing utility.

## MVP Scope
- **Feature Creep**: The parser may try to do too much beyond its core functionality.
  - *Mitigation*: Focus on essential features first, clearly document non-goals.
  - *Mitigation*: Implement PostgreSQL integration first, defer MongoDB for future versions.

- **Schema Evolution**: The Docling schema may change or have undocumented variations.
  - *Mitigation*: Build in some flexibility and version detection where possible.
  - *Mitigation*: Include validation for expected schema structures.

## Resource Constraints
- **Processing Efficiency**: Large documents with many images may be resource-intensive.
  - *Mitigation*: Implement progress tracking and consider chunked processing for large files.
  - *Mitigation*: Add optional limits on maximum document size or image count.

- **Storage Requirements**: External image storage needs may be significant for document-heavy workflows.
  - *Mitigation*: Make storage location configurable and document space requirements.
  - *Mitigation*: Consider compression options for large images.

- **Database Load**: Large batch insertions may impact database performance.
  - *Mitigation*: Implement configurable batch sizes for database operations.
  - *Mitigation*: Add transaction management for reliable operation.

# Appendix

## Functional Requirements Details
The parser meets 30+ specific functional requirements including:
- Accepting DoclingDocument v1.3.0 JSON as input
- Processing PDF documents using the docling library
- Resolving $ref pointers to build an internal representation
- Identifying furniture elements by content_layer attribute
- Extracting text content from furniture elements
- Determining correct reading order of main body content
- Generating hierarchical breadcrumbs using full header text
- Extracting text, table (as formatted grid), and image content
- Using OCR to extract text from images
- Formatting text blocks with preceding text, image text, and succeeding text
- Finding captions and context snippets for tables and images
- Extracting metadata including filename, page_no, and bbox
- Converting bbox coordinates to integers for database storage
- Decoding base64 image data and saving to external location
- Creating a rich metadata object containing all contextual information
- Connecting to and inserting data into a PostgreSQL database
- Supporting configurable database connection parameters
- Validating database schema compatibility
- Managing database transactions for reliability

## Sample Input Structure
The parser expects DoclingDocument v1.3.0 JSON with the following key structures:
- `texts`: List of text elements with labels like "section_header", "text", "list_item"
- `pictures`: List of image elements with base64-encoded data URIs
- `tables`: List of table elements with grid data
- `body`: The main document structure with ordered element references
- `furniture`: Document elements that aren't part of the main content
- `prov`: Provenance information including page numbers and bounding boxes
- Each element contains a self_ref and may reference other elements via $ref

## Database Mapping
The output JSON chunk fields map exactly to the `fusa_library` table columns as follows:
- `_id`: Generated by DB (set to null)
- `block_id`: Sequential ID within document's body chunks
- `doc_id`: External context from input or filename (set to null if not provided)
- `content_type`: "text", "table", or "image"
- `file_type`: From source_metadata.mimetype
- `master_index`: Page number from metadata.page_no
- `master_index2`: Set to null
- `coords_x`: Bbox top-left x coordinate (integer)
- `coords_y`: Bbox top-left y coordinate (integer)
- `coords_cx`: Bbox width (integer)
- `coords_cy`: Bbox height (integer)
- `author_or_speaker`: Set to null
- `added_to_collection`: Set to null
- `file_source`: From source_metadata.filename
- `table_block`: JSON string of table grid for tables, null otherwise
- `modified_date`: Set to null
- `created_date`: Set to null
- `creator_tool`: Fixed value "DoclingToJsonScript_V1.1"
- `external_files`: Relative or absolute path to saved image for images, null otherwise
- `text_block`: Breadcrumb + content text or placeholder for images/tables
  - For images: includes preceding text, image text (from OCR), and succeeding text in markdown format
- `header_text`: Breadcrumb string
- `text_search`: Raw text content or caption for search indexing
- `user_tags`: Set to null
- `special_field1`: JSON string of complete metadata object
- `special_field2`: Breadcrumb string
- `special_field3`: Set to null
- `graph_status`: Set to null
- `dialog`: Set to null
- `embedding_flags`: Set to null

## Output Structure Details
The output JSON has the following structure:
```json
{
      "_id": null,
      "block_id": 1,
      "doc_id": null,
      "content_type": "text|table|image",
      "file_type": "application/pdf",
      "master_index": 1,
      "master_index2": null,
      "coords_x": 56,
      "coords_y": 115,
      "coords_cx": 499,
      "coords_cy": 194,
      "author_or_speaker": null,
      "added_to_collection": null,
      "file_source": "example.pdf",
      "table_block": "[[\"row1col1\", \"row1col2\"], [\"row2col1\", \"row2col2\"]]",
      "modified_date": null,
      "created_date": null,
      "creator_tool": "DoclingToJsonScript_V1.1",
      "external_files": "/path/to/images/doc_1.png",
      "text_block": "Document > Section > Subsection\n\nPreceding text\n\n[Image Text: Text extracted from image via OCR]\n\nSucceeding text",
      "header_text": "Document > Section > Subsection",
      "text_search": "Raw content text or caption",
      "user_tags": null,
      "special_field1": "{\"breadcrumb\": \"...\", \"page_no\": 1, ...}",
      "special_field2": "Document > Section > Subsection",
      "special_field3": null,
      "graph_status": null,
      "dialog": null,
      "embedding_flags": null,
      "metadata": {
        "breadcrumb": "Document > Section > Subsection",
        "page_no": 1,
        "bbox_raw": {"l": 56, "t": 115, "r": 555, "b": 309},
        "caption": "Optional caption text",
        "context_before": "Text appearing before this element (a paragraph  or 100 words, whichever is larger)",
        "context_after": "Text appearing after this element (a paragraph  or 100 words, whichever is larger)",
        "docling_label": "text|table|picture",
        "docling_ref": "#/texts/0",
        "image_mimetype": "image/png",
        "image_width": 499,
        "image_height": 194,
        "image_ocr_text": "Text extracted from the image via OCR"
      }

```

## Code Structure
The parser is organized into the following components:
- **Entry Point**
  - `parse_main.py`: Main program for PDF document processing
  
- **Helper Functions**
  - `build_element_map()`: Creates a map from self_ref to element object
  - `get_flattened_body_sequence()`: Creates ordered list of elements
  - `get_hierarchical_breadcrumb()`: Builds section path for an element
  - `find_sibling_text_in_sequence()`: Gets context before/after an element
  - `get_captions()`: Extracts caption text for tables and images
  - `format_table_content()`: Converts table data to grid format
  - `convert_bbox()`: Transforms bbox coordinates to integers
  - `save_image_externally()`: Decodes and saves image data
  - `extract_image_text()`: Performs OCR on images to extract text

- **Database Interface**
  - `db_adapter.py`: Abstract base class for database adapters
  - `postgres_adapter.py`: PostgreSQL implementation of database adapter 
  - `mongo_adapter.py`: Future MongoDB implementation of database adapter

- **Main Processing Function**
  - `process_docling_json_to_sql_format()`: Orchestrates the entire parsing process

- **Command-line Interface**
  - Argument parsing for input file, output file, image directory, and doc_id
  - Database connection configuration options
  - File reading and error handling
  - Output formatting and writing

## Non-Functional Requirements
The parser implementation addresses several non-functional requirements:
- Written in Python 3.x
- Graceful error handling for file operations and database connections
- Well-commented code with clear structure
- Configurability for image storage location and database connection
- Reasonable performance for typical document sizes
- OCR configurability for different quality thresholds
- Database-agnostic design with adapter pattern for future extensibility

## Implementation Considerations
- The parser strictly follows the DoclingDocument v1.3.0 schema
- Image paths are configurable to support different deployment scenarios
- OCR is used to extract text from images when available
- Text blocks for images include preceding text, image text, and succeeding text
- Breadcrumbs are never truncated, preserving full header text
- Context snippets are limited to a paragraph before and after or 100 words, whichever is larger  
- The tool supports both JSON output and direct database insertion
- Initial database implementation focuses on PostgreSQL with the fusa_library schema
- Future MongoDB support will preserve the same logical structure in document format
- Database connections are configurable via command-line options
- All mandatory fields required by the database schema are populated
- Special attention is given to maintaining document structure and hierarchy
</PRD>
