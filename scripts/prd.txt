Okay, here is the updated Product Requirements Document (PRD) incorporating Option A (External Image Storage) and ensuring breadcrumbs are not truncated.

---

## Product Requirements Document: Docling JSON Parser

**Version:** 1.1
**Date:** 2023-10-27
**Author:** AI Assistant
**Status:** Draft

**1. Introduction**

This document describes the requirements for a Python tool designed to parse JSON files generated by the Docling service (specifically conforming to the DoclingDocument v1.3.0 schema). The tool will extract structured content chunks (text paragraphs, tables, images), identify and separate non-content elements ("furniture"), enrich the content chunks with metadata and context (hierarchical breadcrumbs, surrounding text, captions), save image data to an external location, and format the output into a specific JSON structure suitable for ingestion into the `fusa_library` PostgreSQL database.

**2. Goals**

*   Parse DoclingDocument v1.3.0 JSON input files accurately.
*   Identify and extract main body content (text, tables, images) while separating furniture elements.
*   Generate a hierarchical, non-truncated breadcrumb for each content chunk based on preceding section headers.
*   For each chunk, extract relevant metadata including source filename, page number, and bounding box coordinates.
*   For table and image chunks, extract associated captions.
*   For table and image chunks, extract the preceding and succeeding text paragraphs (or snippets) from the document flow.
*   **Decode image data URIs and save images as binary files to a configurable external location.**
*   Structure the extracted information into a defined JSON output format.
*   Map the output JSON fields clearly to the target `fusa_library` PostgreSQL table schema, storing the *path* to the external image file for image chunks.
*   Provide the core processing logic as Python code.

**3. Non-Goals**

*   Direct interaction with the PostgreSQL database (insertion/updates).
*   Inferring semantic meaning beyond the labels provided in the Docling JSON.
*   Advanced Natural Language Processing (NLP) on the extracted text.
*   Handling Docling JSON schema versions other than v1.3.0.
*   Image manipulation or OCR (assumes image data URI is provided).
*   Complex table structure reconstruction beyond extracting cell text into a grid.
*   Error handling for invalid input JSON beyond basic file/JSON parsing errors.
*   **Management of the external image storage location** (e.g., permissions, backups, cleanup). This is assumed to be handled by infrastructure outside the parser tool itself.

**4. Requirements**

**4.1. Functional Requirements**

| ID   | Requirement Description                                                                                                                                                                                                                            | Priority |
| :--- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------- |
| FR01 | The tool **SHALL** accept a single DoclingDocument v1.3.0 JSON dictionary as input.                                                                                                                                                                | High     |
| FR02 | The tool **SHALL** parse the input JSON, resolving `$ref` pointers to build an internal representation (e.g., an element map).                                                                                                                     | High     |
| FR03 | The tool **SHALL** identify elements belonging to the "furniture" content layer based on the `content_layer` attribute.                                                                                                                            | High     |
| FR04 | The tool **SHALL** extract the text content of all identified "furniture" elements.                                                                                                                                                                | High     |
| FR05 | The tool **SHALL** determine the correct sequential reading order of the main body content elements, respecting nested structures like lists (`groups`). This will form the basis for context extraction.                                           | High     |
| FR06 | The tool **SHALL** iterate through the ordered sequence of main body content elements (excluding furniture).                                                                                                                                         | High     |
| FR07 | For each body content element, the tool **SHALL** determine its `content_type` as "text", "table", or "image" based on its `label` attribute (`section_header`, `list_item`, `text` map to "text").                                                 | High     |
| FR08 | For each body content element, the tool **SHALL** generate a hierarchical `breadcrumb` string by scanning *backward* through the document sequence to find the most recently preceding `section_header` elements at each level, using their **full text**. | High     |
| FR09 | For "text" type elements, the tool **SHALL** extract the `text` content.                                                                                                                                                                            | High     |
| FR10 | For "table" type elements, the tool **SHALL** extract the table data, format it as a list of lists representing the grid text, and then serialize this into a JSON string.                                                                          | High     |
| FR11 | For "image" type elements, the tool **SHALL** extract the image data URI (`image.uri`).                                                                                                                                                              | High     |
| FR12 | For "table" and "image" type elements, the tool **SHALL** extract the text content of any associated `captions` elements.                                                                                                                            | High     |
| FR13 | For "table" and "image" type elements, the tool **SHALL** find the text content (or a placeholder for non-text elements like other images/tables) of the immediately preceding and succeeding non-furniture elements in the document sequence. Limit context snippets to 50 characters each. | High     |
| FR14 | For *all* body content elements, the tool **SHALL** extract metadata including source `filename`, `page_no`, and `bbox` coordinates from the provenance (`prov`) information.                                                                          | High     |
| FR15 | The tool **SHALL** convert the `bbox` coordinates into integer-based `coords_x` (top-left x), `coords_y` (top-left y), `coords_cx` (width), and `coords_cy` (height).                                                                               | Medium   |
| FR16 | **NEW:** For "image" type elements, the tool **SHALL** decode the base64 data URI found in `image.uri`.                                                                                                                                             | High     |
| FR17 | **NEW:** The tool **SHALL** accept a configurable base directory path for storing extracted image files.                                                                                                                                           | High     |
| FR18 | **NEW:** For each decoded image, the tool **SHALL** generate a unique filename (e.g., using doc ID and block ID) and save the binary image data to the configured storage directory. The original file extension **SHOULD** be preserved if possible (derived from mimetype). | High     |
| FR19 | The tool **SHALL** assemble the extracted data for each body chunk into a dictionary structure mapping to the `fusa_library` table schema.                                                                                                             | High     |
| FR20 | The tool **SHALL** create a nested `metadata` object within each chunk dictionary to store richer contextual information (raw bbox, caption, context_before/after snippets, docling label/ref, page number, breadcrumb).                              | High     |
| FR21 | The tool **SHALL** populate the `text_block` field with the breadcrumb prepended to the core text content (or a placeholder/caption for images/tables).                                                                                              | High     |
| FR22 | The tool **SHALL** populate the `table_block` field with the serialized table data (JSON string) for table chunks only.                                                                                                                              | High     |
| FR23 | **UPDATED:** The tool **SHALL** populate the `external_files` field with the **file path** (relative or absolute, depending on configuration) to the saved image file for image chunks only. Set to `null` otherwise.                                  | High     |
| FR24 | The tool **SHALL** populate the `text_search` field with the raw text content for text chunks (or potentially captions for images/tables) to aid potential full-text search indexing.                                                                | Medium   |
| FR25 | The tool **SHALL** populate `special_field1` with the *serialized JSON string* of the entire `metadata` object created in FR20.                                                                                                                     | High     |
| FR26 | The tool **SHALL** populate `special_field2` with the `breadcrumb` string generated in FR08.                                                                                                                                                        | High     |
| FR27 | The tool **SHALL** structure the final output as a JSON object containing a `chunks` list (the processed body chunks), a `furniture` list (strings), and `source_metadata`.                                                                          | High     |
| FR28 | The tool **SHALL** output the final structured data as a JSON formatted string.                                                                                                                                                                      | High     |

**4.2. Input Specification**

*   **Format:** JSON dictionary object.
*   **Schema:** Conforming to DoclingDocument v1.3.0.
*   **Key Structures Used:** (Same as before, including `image.uri`, `image.mimetype`, `image.size`).
*   **Example Input JSON:** (Same as provided previously).

**4.3. Output Specification**

*   **Format:** JSON object.
*   **Top-Level Keys:**
    *   `chunks`: (List) An ordered list of dictionaries, each representing a main body content chunk (text, table, image).
    *   `furniture`: (List) A list of strings, each being the text content of a furniture element.
    *   `source_metadata`: (Object) Contains basic metadata from the input `origin` field.
*   **Chunk Object Structure (within `chunks` list):**
    ```json
    {
        "_id": null,
        "block_id": <integer>,
        "doc_id": null, // Requires external context
        "content_type": "<'text'|'table'|'image'>",
        "file_type": "<string>", // e.g., "application/pdf"
        "master_index": <integer>, // Page number
        "master_index2": null,
        "coords_x": <integer|null>,
        "coords_y": <integer|null>,
        "coords_cx": <integer|null>, // width
        "coords_cy": <integer|null>, // height
        "author_or_speaker": null,
        "added_to_collection": null,
        "file_source": "<string>", // Original filename
        "table_block": "<string|null>", // JSON string of table grid, or null
        "modified_date": null,
        "created_date": null,
        "creator_tool": "DoclingToJsonScript_V1.1", // Updated version
        "external_files": "<string|null>", // *PATH* to saved image file, or null
        "text_block": "<string>", // Breadcrumb + main content/placeholder/caption
        "header_text": "<string>", // Breadcrumb string
        "text_search": "<string|null>", // Raw text or caption for tsvector
        "user_tags": null,
        "special_field1": "<string>", // JSON string representation of metadata object
        "special_field2": "<string>", // Breadcrumb string
        "special_field3": null,
        "graph_status": null,
        "dialog": null,
        "embedding_flags": null,
        "metadata": { // Richer context/metadata
            "breadcrumb": "<string>", // Full breadcrumb
            "page_no": <integer>,
            "bbox_raw": { /* Original bbox object */ },
            "caption": "<string|null>",
            "context_before_50_chars": "<string|null>",
            "context_after_50_chars": "<string|null>",
            "docling_label": "<string>",
            "docling_ref": "<string>",
            // Potentially add image metadata extracted during saving
            "image_mimetype": "<string|null>", // (For image chunks)
            "image_width": "<integer|null>", // (For image chunks)
            "image_height": "<integer|null>" // (For image chunks)
        }
    }
    ```
*   **Example Output JSON (Conceptual, reflecting Option A):**
    ```json
    {
      "chunks": [
        {
          // ... table chunk as before, external_files is null ...
          "_id": null, "block_id": 1, "doc_id": null, "content_type": "table", "file_type": "application/pdf",
          "master_index": 1, "coords_x": 56, "coords_y": 115, "coords_cx": 499, "coords_cy": 194,
          "file_source": "SBW_AI sample page10-11.pdf",
          "table_block": "[[\"MCU\", \"Micro controller unit\"], [\"MPS\", \"Motor position sensor\"], ...]",
          "external_files": null,
          "text_block": "Document > 3. INTRODUCTION > 3.2. Terms and Definitions > 3.2.1. Abbreviations\n\n[Table]", // Using placeholder now for non-text content
          "header_text": "Document > 3. INTRODUCTION > 3.2. Terms and Definitions > 3.2.1. Abbreviations",
          "text_search": "[Table]", // Or caption if available
          "special_field1": "{\"breadcrumb\": \"Document > 3. INTRODUCTION > 3.2. Terms and Definitions > 3.2.1. Abbreviations\", ... , \"docling_ref\": \"#/tables/0\"}",
          "special_field2": "Document > 3. INTRODUCTION > 3.2. Terms and Definitions > 3.2.1. Abbreviations",
          "metadata": {
             "breadcrumb": "Document > 3. INTRODUCTION > 3.2. Terms and Definitions > 3.2.1. Abbreviations",
             "page_no": 1, "bbox_raw": {...}, "caption": null, "context_before_50_chars": null, "context_after_50_chars": "3.2.2. Definitions",
             "docling_label": "table", "docling_ref": "#/tables/0"
          }
        },
        {
          // ... text chunk as before ...
          "_id": null, "block_id": 2, "doc_id": null, "content_type": "text", "file_type": "application/pdf",
          "master_index": 1, "coords_x": 74, "coords_y": 438, "coords_cx": 150, "coords_cy": 15,
          "file_source": "SBW_AI sample page10-11.pdf",
          "table_block": null, "external_files": null,
          "text_block": "Document > 3. INTRODUCTION > 3.2. Terms and Definitions\n\n3.2.2. Definitions",
          "header_text": "Document > 3. INTRODUCTION > 3.2. Terms and Definitions",
          "text_search": "3.2.2. Definitions",
          "special_field1": "{\"breadcrumb\": \"Document > 3. INTRODUCTION > 3.2. Terms and Definitions\", ... ,\"context_before_50_chars\": \"[Table]\", \"context_after_50_chars\": \"Requirements under this section are not applicable \", ...}",
          "special_field2": "Document > 3. INTRODUCTION > 3.2. Terms and Definitions",
          "metadata": {
             "breadcrumb": "Document > 3. INTRODUCTION > 3.2. Terms and Definitions",
             "page_no": 1, "bbox_raw": {...}, "caption": null, "context_before_50_chars": "[Table]", "context_after_50_chars": "Requirements under this section are not applicable ",
             "docling_label": "section_header", "docling_ref": "#/texts/1"
          }
        },
        // ... more chunks ...
        {
          "_id": null,
          "block_id": N,
          "doc_id": null,
          "content_type": "image",
          "file_type": "application/pdf",
          "master_index": 2,
          "coords_x": 392, "coords_y": 345, "coords_cx": 148, "coords_cy": 107,
          "file_source": "SBW_AI sample page10-11.pdf",
          "table_block": null,
          // *** UPDATED external_files ***
          "external_files": "/path/to/your/image/storage/unknown_doc_N.png", // Path where image was saved
          "text_block": "Document > 4. LAYER 1 STAKEHOLDER & VEHICLE REQUIREMENTS > 4.1. Feature Definition (VOP) > Functional Needs\n\n[Image]",
          "header_text": "Document > 4. LAYER 1 STAKEHOLDER & VEHICLE REQUIREMENTS > 4.1. Feature Definition (VOP) > Functional Needs",
          "text_search": "[Image]", // Or caption if available
          "special_field1": "{\"breadcrumb\": \"...\", \"page_no\": 2, ..., \"caption\": null, \"context_before_50_chars\": \"...compensation.\", \"context_after_50_chars\": \"Benefits and Competitive Solutions:\", ..., \"image_mimetype\": \"image/png\", \"image_width\": 296, \"image_height\": 215}", // Added image metadata
          "special_field2": "Document > 4. LAYER 1 STAKEHOLDER & VEHICLE REQUIREMENTS > 4.1. Feature Definition (VOP) > Functional Needs",
          "metadata": {
             "breadcrumb": "Document > 4. LAYER 1 STAKEHOLDER & VEHICLE REQUIREMENTS > 4.1. Feature Definition (VOP) > Functional Needs",
             "page_no": 2, "bbox_raw": {...}, "caption": null, "context_before_50_chars": "...compensation.", "context_after_50_chars": "Benefits and Competitive Solutions:",
             "docling_label": "picture", "docling_ref": "#/pictures/0",
             // Added image metadata
             "image_mimetype": "image/png", "image_width": 296, "image_height": 215
          }
        }
      ],
      "furniture": [ /* ... list of furniture strings ... */ ],
      "source_metadata": { /* ... source file info ... */ }
    }
    ```

**4.4. Database Mapping (`fusa_library` Table)**

| Output JSON Chunk Field | `fusa_library` Column | Notes                                                                                                          |
| :---------------------- | :-------------------- | :------------------------------------------------------------------------------------------------------------- |
| `_id`                   | `_id`                 | Generated by DB. `null`.                                                                                       |
| `block_id`              | `block_id`            | Sequential ID within the document's *body* chunks.                                                             |
| `doc_id`                | `doc_id`              | External context. `null`.                                                                                      |
| `content_type`          | `content_type`        | "text", "table", or "image".                                                                                   |
| `file_type`             | `file_type`           | From `source_metadata.mimetype`.                                                                               |
| `metadata.page_no`      | `master_index`        | Page number.                                                                                                   |
| `null`                  | `master_index2`       | `null`.                                                                                                        |
| `coords_x`              | `coords_x`            | Bbox top-left x.                                                                                               |
| `coords_y`              | `coords_y`            | Bbox top-left y.                                                                                               |
| `coords_cx`             | `coords_cx`           | Bbox width.                                                                                                    |
| `coords_cy`             | `coords_cy`           | Bbox height.                                                                                                   |
| `null`                  | `author_or_speaker`   | `null`.                                                                                                        |
| `null`                  | `added_to_collection` | `null`.                                                                                                        |
| `file_source`           | `file_source`         | From `source_metadata.filename`.                                                                               |
| `table_block`           | `table_block`         | JSON string of table grid (for tables). `null` otherwise.                                                      |
| `null`                  | `modified_date`       | `null`.                                                                                                        |
| `null`                  | `created_date`        | `null`.                                                                                                        |
| `creator_tool`          | `creator_tool`        | Constant "DoclingToJsonScript_V1.1".                                                                           |
| **`external_files`**    | **`external_files`**  | **File path/URL** to saved image (for images). `null` otherwise.                                               |
| `text_block`            | `text_block`          | Breadcrumb + content/placeholder/caption.                                                                      |
| `header_text`           | `header_text`         | Breadcrumb string.                                                                                             |
| `text_search`           | `text_search`         | Raw text or caption.                                                                                           |
| `null`                  | `user_tags`           | `null`.                                                                                                        |
| **`metadata` (JSON str)** | **`special_field1`**| **Serialized JSON of the nested `metadata` object.**                                                           |
| **`breadcrumb` string** | **`special_field2`**  | **The breadcrumb string.**                                                                                     |
| `null`                  | `special_field3`      | `null`.                                                                                                        |
| `null`                  | `graph_status`        | `null`.                                                                                                        |
| `null`                  | `dialog`              | `null`.                                                                                                        |
| `null`                  | `embedding_flags`     | `null`.                                                                                                        |
| *N/A*                   | `ts`                  | DB generated from `text_search`.                                                                               |

**4.5. Non-Functional Requirements**

| ID   | Requirement Description                                                                                       | Priority |
| :--- | :------------------------------------------------------------------------------------------------------------ | :------- |
| NF01 | The script **SHALL** be written in Python 3.x.                                                                | High     |
| NF02 | The script **SHALL** handle basic file reading and JSON parsing errors gracefully.                            | Medium   |
| NF03 | The code **SHOULD** be reasonably commented.                                                                  | Medium   |
| NF04 | The script **SHALL** accept the base path for image storage as a command-line argument or configuration variable. | High     |
| NF05 | The script **SHALL** handle potential errors during image decoding or file saving (e.g., log warning, skip image). | Medium   |
| NF06 | Performance **SHOULD** be reasonable for typical document sizes.                                              | Low      |

**5. Open Questions / Future Considerations**

*   How should `doc_id` be determined and passed to the script?
*   What is the exact desired format/convention for the image file path stored in `external_files` (absolute vs. relative, URL)?
*   Refine the `text_search` content for tables (e.g., include cell text instead of just caption).
*   Error handling strategy for missing optional keys (like `level` on headers).
*   Should the script create subdirectories within the image storage path based on `doc_id`?
*   How to handle potential filename collisions if `doc_id` and `block_id` are not unique across all documents processed? (Using UUIDs might be safer).
*   Is storing the full metadata JSON in `special_field1` acceptable, or should more fields be mapped directly if the schema evolves?

**6. Draft Code**

*(The refined Python code from the previous response, incorporating Option A logic for saving images externally and non-truncated breadcrumbs, should be included here)*

```python
# ==========================================
# Docling JSON Parser - Draft Code V1.1 (External Images)
# ==========================================
import json
import base64
import os
import re
import argparse # For command-line arguments

# --- Helper Functions ---

def build_element_map(doc_data):
    # (Implementation from previous response)
    """Creates a map from self_ref to element object."""
    element_map = {}
    element_map['#/body'] = doc_data.get('body', {})
    element_map['#/furniture'] = doc_data.get('furniture', {})

    for key in ['texts', 'pictures', 'tables', 'groups']:
        if key in doc_data:
            for element in doc_data[key]:
                if 'self_ref' in element:
                    element_map[element['self_ref']] = element
    return element_map


def _flatten_sequence_recursive(element_ref, element_map, sequence_list):
    # (Implementation from previous response)
    """Recursive helper to flatten the body sequence."""
    element = element_map.get(element_ref)
    if not element:
        return

    if element.get('name') != 'list':
         sequence_list.append(element_ref)

    if 'children' in element and isinstance(element['children'], list):
        for child_ref_obj in element['children']:
            child_ref = child_ref_obj.get('$ref')
            if child_ref:
                 _flatten_sequence_recursive(child_ref, element_map, sequence_list)

def get_flattened_body_sequence(doc_data, element_map):
    # (Implementation from previous response)
    """Creates a flattened, ordered list of all element $refs under body."""
    flattened_sequence = []
    body_ref = '#/body'
    _flatten_sequence_recursive(body_ref, element_map, flattened_sequence)
    return [ref for ref in flattened_sequence if ref != '#/body']


def get_hierarchical_breadcrumb(element_ref, flattened_sequence, element_map):
    # (Implementation using full header text - see previous response)
    """Builds a breadcrumb reflecting the section hierarchy."""
    path_parts = []
    last_header_at_level = {}

    try:
        current_index = -1
        for idx, ref in enumerate(flattened_sequence):
            if ref == element_ref:
                current_index = idx
                break
        if current_index == -1:
             elem = element_map.get(element_ref)
             parent_ref = elem.get('parent', {}).get('$ref') if elem else None
             if parent_ref and parent_ref != '#/body':
                 # Try getting breadcrumb from parent if element itself not in sequence
                 # This might happen for elements only referenced by captions, though less likely now
                 parent_breadcrumb = get_hierarchical_breadcrumb(parent_ref, flattened_sequence, element_map)
                 elem_label = elem.get('label', 'Element').capitalize() if elem else 'Element'
                 return f"{parent_breadcrumb} > [{elem_label}]" # Indicate it's the element itself
             else:
                 return "Document"
    except ValueError:
         return "Document"

    for i in range(current_index, -1, -1):
        ref = flattened_sequence[i]
        element = element_map.get(ref)
        if not element: continue

        if element.get('label') == 'section_header':
            level = element.get('level', 1)
            if level not in last_header_at_level:
                header_text = element.get('text', f'Header L{level}')
                last_header_at_level[level] = header_text.strip()

    path_parts.append("Document")
    for level in sorted(last_header_at_level.keys()):
        path_parts.append(last_header_at_level[level])

    return " > ".join(path_parts)


def find_sibling_text_in_sequence(element_ref, flattened_sequence, element_map, direction='previous', max_chars=50):
    # (Implementation using placeholders for non-text - see previous response)
    """Finds text/placeholder of nearest previous/next non-furniture element."""
    try:
        current_index = -1
        for idx, ref in enumerate(flattened_sequence):
             if ref == element_ref:
                 current_index = idx
                 break
        if current_index == -1: return None
    except ValueError:
        return None

    step = -1 if direction == 'previous' else 1
    search_index = current_index + step

    while 0 <= search_index < len(flattened_sequence):
        sibling_ref = flattened_sequence[search_index]
        sibling_element = element_map.get(sibling_ref)

        if sibling_element and sibling_element.get('content_layer') != 'furniture':
            if sibling_element.get('label') in ['text', 'section_header', 'list_item']:
                full_text = sibling_element.get('text', '')
                return full_text[-max_chars:] if direction == 'previous' else full_text[:max_chars]
            elif sibling_element.get('label') in ['table', 'image']:
                 caption = get_captions(sibling_element, element_map)
                 placeholder = f"[{sibling_element.get('label', 'element').capitalize()}]"
                 context_text = f"{placeholder} {caption}" if caption else placeholder
                 return context_text[-max_chars:] if direction == 'previous' else context_text[:max_chars]

        search_index += step
    return None

def get_context_from_sequence(element_ref, flattened_sequence, element_map, max_chars=50):
     # (Implementation from previous response)
     """Gets previous/next context."""
     prev_text = find_sibling_text_in_sequence(element_ref, flattened_sequence, element_map, direction='previous', max_chars=max_chars)
     next_text = find_sibling_text_in_sequence(element_ref, flattened_sequence, element_map, direction='next', max_chars=max_chars)
     return {
         'context_before': prev_text,
         'context_after': next_text
     }

def get_captions(element, element_map):
    # (Implementation from previous response)
    """Extracts captions."""
    captions_text = []
    if 'captions' in element and isinstance(element['captions'], list):
        for caption_ref_obj in element['captions']:
            caption_ref = caption_ref_obj.get('$ref')
            if caption_ref:
                caption_element = element_map.get(caption_ref)
                if caption_element and caption_element.get('label') == 'text':
                    captions_text.append(caption_element.get('text', ''))
    return " ".join(captions_text).strip() or None

def format_table_content(table_element):
    # (Implementation from previous response)
    """Formats table grid text."""
    data = table_element.get('data', {})
    grid = data.get('grid')
    if not grid: return None
    formatted_table = []
    for row in grid:
        formatted_row = []
        processed_in_row = set()
        for col_idx, cell_obj in enumerate(row):
             if col_idx in processed_in_row: continue
             if cell_obj and isinstance(cell_obj, dict):
                 text = cell_obj.get('text', '')
                 col_span = cell_obj.get('col_span', 1)
                 formatted_row.append(text)
                 for i in range(1, col_span):
                     processed_in_row.add(col_idx + i)
             else:
                 formatted_row.append('')
        formatted_table.append(formatted_row)
    return formatted_table


def convert_bbox(prov_entry):
    # (Implementation from previous response)
    """Converts bbox."""
    if not prov_entry or 'bbox' not in prov_entry: return None, None, None, None
    bbox = prov_entry['bbox']
    l, t, r, b = bbox.get('l'), bbox.get('t'), bbox.get('r'), bbox.get('b')
    if None in [l, t, r, b]: return None, None, None, None
    width = r - l
    height = abs(t - b)
    x = l
    y = min(t, b)
    return int(x), int(y), int(width), int(height)

def save_image_externally(image_uri, image_storage_dir, doc_id_for_file, block_id):
    """Decodes base64 URI and saves image to external directory."""
    if not image_uri or not image_uri.startswith('data:image'):
        return None, None, None, None # Return None for path and metadata if no URI

    try:
        # Extract mimetype and base64 data
        header, base64_data = image_uri.split(',', 1)
        mimetype_match = re.match(r'data:(image/\w+);base64', header)
        image_mimetype = mimetype_match.group(1) if mimetype_match else 'image/png'
        image_ext = image_mimetype.split('/')[-1]
        image_binary = base64.b64decode(base64_data)

        # Ensure directory exists
        os.makedirs(image_storage_dir, exist_ok=True)

        # Define unique filename (ensure doc_id is a string)
        filename_base = f"{str(doc_id_for_file)}_{block_id}"
        image_filename = os.path.join(image_storage_dir, f"{filename_base}.{image_ext}")
        counter = 1
        # Basic collision handling (append number if file exists)
        while os.path.exists(image_filename):
            image_filename = os.path.join(image_storage_dir, f"{filename_base}_{counter}.{image_ext}")
            counter += 1

        with open(image_filename, 'wb') as img_file:
            img_file.write(image_binary)
        print(f"Saved image to: {image_filename}")
        # Placeholder for width/height extraction if needed later from image_binary
        # using libraries like Pillow (PIL)
        width, height = None, None # Requires image library
        return image_filename, image_mimetype, width, height

    except Exception as e:
        print(f"Error decoding/saving image: {e}")
        return None, None, None, None


# --- Main Processing Function ---
def process_docling_json_to_sql_format(doc_data, image_dir, doc_id=None):
    """Processes the DoclingDocument dictionary into the target JSON format."""

    element_map = build_element_map(doc_data)
    flattened_sequence = get_flattened_body_sequence(doc_data, element_map)

    # Use filename as doc_id if not provided
    if doc_id is None:
        doc_id = doc_data.get('origin', {}).get('filename', 'unknown_doc')

    output_data = {
        "chunks": [],
        "furniture": [],
        "source_metadata": {
             "filename": doc_data.get('origin', {}).get('filename'),
             "mimetype": doc_data.get('origin', {}).get('mimetype'),
             "binary_hash": doc_data.get('origin', {}).get('binary_hash')
        }
    }

    # 1. Process Furniture
    for text_element in doc_data.get('texts', []):
        if text_element.get('content_layer') == 'furniture':
            output_data["furniture"].append(text_element.get('text', ''))

    # 2. Process Body Chunks
    chunk_counter = 0
    for element_ref in flattened_sequence:
        element = element_map.get(element_ref)
        if not element or element.get('content_layer') == 'furniture':
            continue

        chunk_counter += 1
        label = element.get('label', 'unknown')
        breadcrumb = get_hierarchical_breadcrumb(element_ref, flattened_sequence, element_map)
        content_type = 'unknown'
        core_content_str = None
        table_block_content = None
        image_path = None # Changed from image_uri
        text_block_content = None
        text_search_content = None
        img_meta_mime = None
        img_meta_w = None
        img_meta_h = None

        if label in ['text', 'section_header', 'list_item']:
            content_type = 'text'
            core_content_str = element.get('text', '')
            text_block_content = f"{breadcrumb}\n\n{core_content_str}"
            text_search_content = core_content_str
        elif label == 'table':
            content_type = 'table'
            core_content_list = format_table_content(element)
            table_block_content = json.dumps(core_content_list) if core_content_list else None
            caption_content = get_captions(element, element_map) or "[Table]"
            text_block_content = f"{breadcrumb}\n\n{caption_content}"
            text_search_content = caption_content # Or join cell text?
        elif label == 'picture':
            content_type = 'image'
            image_uri = element.get('image', {}).get('uri')
            # Save image externally and get path/metadata
            image_path, img_meta_mime, img_meta_w, img_meta_h = save_image_externally(
                image_uri, image_dir, doc_id, chunk_counter
            )
            caption_content = get_captions(element, element_map) or "[Image]"
            text_block_content = f"{breadcrumb}\n\n{caption_content}" # Or maybe store path here too?
            text_search_content = caption_content # Search based on caption

        # --- Gather Metadata ---
        prov = element.get('prov', [{}])[0]
        page_no = prov.get('page_no')
        bbox_raw = prov.get('bbox')
        coords_x, coords_y, coords_cx, coords_cy = convert_bbox(prov) or (None, None, None, None)

        caption = get_captions(element, element_map) if content_type in ['table', 'image'] else None
        context = get_context_from_sequence(element_ref, flattened_sequence, element_map, max_chars=50)

        metadata_obj = {
            "breadcrumb": breadcrumb,
            "page_no": page_no,
            "bbox_raw": bbox_raw,
            "caption": caption,
            "context_before_50_chars": context.get('context_before'),
            "context_after_50_chars": context.get('context_after'),
            "docling_label": label,
            "docling_ref": element_ref
        }
        # Add image metadata if available
        if content_type == 'image':
             metadata_obj["image_mimetype"] = img_meta_mime
             metadata_obj["image_width"] = img_meta_w
             metadata_obj["image_height"] = img_meta_h

        # --- Assemble Chunk Object mapping to SQL ---
        chunk_obj = {
            "_id": None, "block_id": chunk_counter, "doc_id": doc_id, # Use passed/derived doc_id
            "content_type": content_type, "file_type": doc_data.get('origin', {}).get('mimetype'),
            "master_index": page_no, "master_index2": None,
            "coords_x": coords_x, "coords_y": coords_y, "coords_cx": coords_cx, "coords_cy": coords_cy,
            "author_or_speaker": None, "added_to_collection": None,
            "file_source": doc_data.get('origin', {}).get('filename'),
            "table_block": table_block_content, "modified_date": None, "created_date": None,
            "creator_tool": "DoclingToJsonScript_V1.1",
            "external_files": image_path, # Store image PATH
            "text_block": text_block_content,
            "header_text": breadcrumb,
            "text_search": text_search_content,
            "user_tags": None,
            "special_field1": json.dumps(metadata_obj), # Store metadata JSON here
            "special_field2": breadcrumb, # Store breadcrumb here
            "special_field3": None, "graph_status": None, "dialog": None, "embedding_flags": None,
            "metadata": metadata_obj # Keep nested object for direct JSON use
        }
        output_data["chunks"].append(chunk_obj)

    return output_data

# --- Argument Parser and Main Execution ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Parse Docling JSON and extract structured chunks.')
    parser.add_argument('input_file', help='Path to the input DoclingDocument JSON file.')
    parser.add_argument('output_file', help='Path to save the output JSON file.')
    parser.add_argument('--image_dir', required=True, help='Base directory path to save extracted images.')
    parser.add_argument('--doc_id', default=None, help='Optional document ID (string) to use for filenames/DB field. Defaults to input filename.')

    args = parser.parse_args()

    input_filepath = args.input_file
    output_filepath = args.output_file
    image_storage_dir = args.image_dir
    doc_id_param = args.doc_id

    input_dict = None
    try:
         with open(input_filepath, 'r', encoding='utf-8') as f:
             input_dict = json.load(f)
    except FileNotFoundError:
         print(f"Error: Input file not found at {input_filepath}")
    except json.JSONDecodeError as e:
         print(f"Error: Invalid JSON in input file {input_filepath}: {e}")
    except Exception as e:
         print(f"An unexpected error occurred loading the input file: {e}")


    if input_dict:
        try:
            # Pass image directory and optional doc_id to the processing function
            output_json_data = process_docling_json_to_sql_format(input_dict, image_storage_dir, doc_id_param)
            output_json_string = json.dumps(output_json_data, indent=2, ensure_ascii=False)

            # Save to output file
            with open(output_filepath, 'w', encoding='utf-8') as outfile:
                outfile.write(output_json_string)
            print(f"Successfully processed data and saved to {output_filepath}")

        except Exception as e:
            print(f"An error occurred during processing: {e}")

```

---